{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiNuuoofvZTgr8vcnqgg8m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rizqinursulistiasari/analisis-sentimen-rohingya/blob/main/1_preprocessing_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PreProcessing**"
      ],
      "metadata": {
        "id": "nVnr-WQRsk4I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hdRFF8sF7Nq3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Membaca file Excel menggunakan pandas\n",
        "nama_file = \"Data Mentah.csv\"  # Ganti dengan nama file yang sesuai\n",
        "Data = pd.read_csv(nama_file, encoding='ISO-8859-1')\n",
        "\n",
        "# Menampilkan beberapa baris pertama dari dataframe\n",
        "Data.head(15000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CH_W_r5l99-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tahap Awal Preprocessing\n",
        "df = pd.DataFrame(Data['Content'])\n",
        "df.head(15000)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cr_Q9wH-9_uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Case Folding\n",
        "df['Case Folding'] = df['Content'].str.lower()\n",
        "df.head(15000)"
      ],
      "metadata": {
        "id": "2j_Les_Z-Fol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "def remove_URL(tweet): #Menghapus URL/Link\n",
        "  url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "  return url.sub(r'', tweet)\n",
        "\n",
        "def remove_emoji(tweet): #Menghapus emoji\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "    u\"\\U0001F600-\\U0001F64F\"\n",
        "    u\"\\U0001F300-\\U0001F5FF\"\n",
        "    u\"\\U0001F680-\\U0001F6FF\"\n",
        "    u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "    u\"\\U000024C2-\\U0001F251\"\n",
        "    u\"\\U0001f926-\\U0001f937\"\n",
        "    u\"\\U00010000-\\U0010ffff\"\n",
        "    u\"\\u2600-\\u2B55\"\n",
        "    u\"\\u200d\"\n",
        "    u\"\\u23cf\"\n",
        "    u\"\\u23e9\"\n",
        "    u\"\\u231a\"\n",
        "    u\"\\ufe0f\"\n",
        "    u\"\\u3030\"\n",
        "                        \"]+\", flags=re.UNICODE)\n",
        "  return emoji_pattern.sub(r'', tweet)\n",
        "\n",
        "def remove_angka_dll(tweet):\n",
        "  tweet = re.sub(r'\\d+', '', tweet) #Menghapus angka\n",
        "  tweet = re.sub(r'\\$\\w*', '', tweet) #Menghapus ticker pasar saham seperti $GE\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet) #Menghapus RT\n",
        "  tweet = re.sub(r'#[^\\s]+', '', tweet) #Menghapus hashtag\n",
        "  tweet = re.sub(r'@[^\\s]+', '', tweet) #Menghapus mention\n",
        "  tweet = re.sub(r'\\n', ' ', tweet) #Menghapus hal baru\n",
        "  tweet = re.sub(r'&amp', '', tweet) #Menghapus tanda &amp\n",
        "  tweet = re.sub(r'[^A-Za-z ]+', ' ', tweet) #Menghapus karakter non alfabet\n",
        "  tweet = tweet.strip()\n",
        "  return tweet\n",
        "\n",
        "df['Cleaning'] = df['Case Folding'].apply(lambda x: remove_URL(x))\n",
        "df['Cleaning'] = df['Cleaning'].apply(lambda x: remove_emoji(x))\n",
        "df['Cleaning'] = df['Cleaning'].apply(lambda x: remove_angka_dll(x))\n",
        "df.head(15000)"
      ],
      "metadata": {
        "id": "oozLB1cQAgTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalisasi\n",
        "kamus_df = pd.read_csv('Kamus Normalisasi Rohingya.csv')\n",
        "\n",
        "# Fungsi untuk normalisasi teks berdasarkan kamus normalisasi\n",
        "def normalisasi(str_text):\n",
        "    for i in norm:\n",
        "        if pd.notna(norm[i]):  # Memeriksa apakah nilai normalisasi tidak NaN\n",
        "            str_text = str_text.replace(i, norm[i])\n",
        "        else:\n",
        "            str_text = str_text.replace(i, '')  # Menghapus kata asal jika normalisasi adalah NaN\n",
        "    return str_text\n",
        "\n",
        "# Membuat kamus normalisasi dari DataFrame kamus_df\n",
        "norm = dict(zip(kamus_df['Kata Asal'], kamus_df['Kata Normalisasi']))\n",
        "\n",
        "# Normalisasi kolom 'Cleaning' pada DataFrame 'df'\n",
        "df['Normalization'] = df['Cleaning'].apply(normalisasi)\n",
        "df.head(15000)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EXOti9erAqLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "df['Tokenization'] = df['Normalization'].apply(lambda x:x.split())\n",
        "df.head(15000)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8cUg1N23BBwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtering/Stopword Removal\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Ambil stopwords dalam bahasa Indonesia (atau sesuai kebutuhan)\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "stop_words.remove('tidak')\n",
        "\n",
        "# Proses filtering tanpa menghapus kata \"tidak\"\n",
        "def remove_stopwords(text):\n",
        "  return [word for word in text if word not in stop_words]\n",
        "\n",
        "df['Filtering'] = df['Tokenization'].apply(lambda x: remove_stopwords(x))\n",
        "df.head(15000)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-f8sN-jcC1TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steamming Data\n",
        "!pip install Sastrawi\n",
        "\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stem_text(text):\n",
        "  return [stemmer.stem(word) for word in text]\n",
        "\n",
        "df['Stemming'] = df['Filtering'].apply(lambda x: ' '.join(stem_text(x)))\n",
        "df.head(15000)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bbYynWILC7q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simpan ke file CSV\n",
        "df.to_csv('Rohingya Preprocessing.csv', encoding='utf8', index=False)"
      ],
      "metadata": {
        "id": "HkSEUP3wC-3b"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}